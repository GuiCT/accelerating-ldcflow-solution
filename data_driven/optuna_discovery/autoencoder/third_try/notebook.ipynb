{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localização da pasta do workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = '/home/guilherme/Projects/accelerating-ldcflow-solution/data_driven'\n",
    "# WORKSPACE_PATH = '/workspace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando módulo utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(WORKSPACE_PATH))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo dados gerados via método numérico\n",
    "\n",
    "Esses dados são armazenados em um arquivo HDF5 (Hierarchical Data Format 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.file_loader import load_data\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "CACHE_FILE_PATH = os.path.join(\n",
    "    WORKSPACE_PATH, 'data', 'cached', 'trial_autoencoder_data.h5')\n",
    "if not os.path.isfile(CACHE_FILE_PATH):\n",
    "    data_file_path = os.path.join(\n",
    "        WORKSPACE_PATH, 'data', 'autoencoder_data.h5')\n",
    "    loaded_data = load_data(data_file_path)\n",
    "    # Tomando uma amostra de 50% para as trials do Optuna\n",
    "    # 10% desses 50% serão para validação, o resto será para treinamento durante as trials\n",
    "    rng = np.random.default_rng()\n",
    "    loaded_data = rng.permuted(loaded_data, axis=0)  # Realiza permutação\n",
    "    trial_size = int(loaded_data.shape[0] * 0.5)\n",
    "    loaded_data = np.resize(loaded_data, (trial_size,) + loaded_data.shape[1:])\n",
    "    training_size = int(trial_size * 0.9)\n",
    "    validation_size = int(trial_size * 0.1)\n",
    "    training_data = np.copy(loaded_data[:training_size, :, :, :])\n",
    "    validation_data = np.copy(loaded_data[training_size:, :, :, :])\n",
    "    del loaded_data\n",
    "    with h5py.File(CACHE_FILE_PATH, 'w') as h5f:\n",
    "        h5f.create_dataset('training', data=training_data)\n",
    "        h5f.create_dataset('validation', data=validation_data)\n",
    "else:\n",
    "    with h5py.File(CACHE_FILE_PATH, 'r') as h5f:\n",
    "        training_data = h5f['training'][:]\n",
    "        validation_data = h5f['validation'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando diferentes modelos de Autoencoder\n",
    "\n",
    "Hiperparâmetros alterados:\n",
    "- Quantidade de camadas de encoder e decoder\n",
    "- Números de neurônios\n",
    "- Funções de ativação para cada layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros constantes\n",
    "original_dim = 63 * 63 * 2\n",
    "epochs = 50  # Deve ser o suficiente pra identificar as maiores discrepâncias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procura de hiperparâmetros utilizando _framework_ Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import Trial\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def objective(trial: Trial):\n",
    "    batch_size = 32\n",
    "    latent_size = trial.suggest_int('latent_size', 8, 64, log=True)\n",
    "    # Latent layer\n",
    "    activation_function_latent = trial.suggest_categorical(\n",
    "        'activation_function_latent', ['selu', 'relu', 'linear', 'tanh', 'sigmoid'])\n",
    "\n",
    "    # Intermediate layers\n",
    "    n_intermediate_layers = trial.suggest_int('n_intermediate_layers', 0, 2)\n",
    "    activation_functions_encoder = []\n",
    "    activation_functions_decoder = []\n",
    "    intermediate_layers_neurons = []\n",
    "    for i in range(n_intermediate_layers):\n",
    "        if i == 0:\n",
    "            maximum = 63 * 63\n",
    "        else:\n",
    "            maximum = intermediate_layers_neurons[i - 1]\n",
    "        intermediate_layers_neurons.append(trial.suggest_int(\n",
    "            f'n_neurons_l{i + 1}', latent_size, maximum, log=True))\n",
    "        activation_functions_encoder.append(trial.suggest_categorical(\n",
    "            f'activation_function_encoder_l{i + 1}', ['selu', 'relu', 'linear', 'tanh', 'sigmoid']))\n",
    "        activation_functions_decoder.append(trial.suggest_categorical(\n",
    "            f'activation_function_decoder_l{i + 1}', ['selu', 'relu', 'linear', 'tanh', 'sigmoid']))\n",
    "\n",
    "    # Last layer\n",
    "    activation_function_last = trial.suggest_categorical(\n",
    "        'activation_function_last', ['selu', 'relu', 'linear', 'tanh', 'sigmoid'])\n",
    "\n",
    "    autoencoder = keras.models.Sequential()\n",
    "    autoencoder.add(layers.Reshape((original_dim,), input_shape=(63, 63, 2)))\n",
    "    for i in range(n_intermediate_layers):\n",
    "        autoencoder.add(layers.Dense(\n",
    "            intermediate_layers_neurons[i], activation=activation_functions_encoder[i]))\n",
    "    autoencoder.add(layers.Dense(\n",
    "        latent_size, activation=activation_function_latent))\n",
    "    for i in range(n_intermediate_layers - 1, -1, -1):\n",
    "        autoencoder.add(layers.Dense(\n",
    "            intermediate_layers_neurons[i], activation=activation_functions_decoder[i]))\n",
    "    autoencoder.add(layers.Dense(\n",
    "        original_dim, activation=activation_function_last))\n",
    "    autoencoder.add(layers.Reshape((63, 63, 2), input_shape=(original_dim,)))\n",
    "    opt = Adam(learning_rate=1e-5)\n",
    "    autoencoder.compile(optimizer=opt, loss='mse')\n",
    "    print(autoencoder.summary())\n",
    "    try:\n",
    "        history = autoencoder.fit(\n",
    "            training_data,\n",
    "            training_data,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(validation_data, validation_data),\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        trial.study.stop()\n",
    "\n",
    "    score = history.history['val_loss'][-1]\n",
    "    autoencoder.save_weights(\n",
    "        f'/workspace/models/autoencoder_2/{trial.number}.h5')\n",
    "    # Checa se score é NaN\n",
    "    # Se sim, retorna maxfloat para desmotivar uso de hiperparâmetros que causem esse comportamento\n",
    "\n",
    "    # Utiliza número de camadas intermediárias como segunda métrica\n",
    "\n",
    "    if (np.isnan(score)):\n",
    "        return np.finfo(np.float32).max, n_intermediate_layers * 2\n",
    "    else:\n",
    "        return score, n_intermediate_layers * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "\n",
    "# Se o estudo já existe no banco de dados, carrega-o, caso contrário, cria um novo\n",
    "DATABASE_PATH = f\"sqlite:///{os.path.join(WORKSPACE_PATH, 'optuna_discovery', 'databases', 'autoencoder_params_3.db')}\"\n",
    "try:\n",
    "    study = optuna.create_study(\n",
    "        study_name='autoencoder_params_3', directions=['minimize', 'minimize'], storage=DATABASE_PATH)\n",
    "except optuna.exceptions.DuplicatedStudyError:\n",
    "    study = optuna.load_study(\n",
    "        study_name='autoencoder_params_3', storage=DATABASE_PATH)\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=60 * 30)\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
