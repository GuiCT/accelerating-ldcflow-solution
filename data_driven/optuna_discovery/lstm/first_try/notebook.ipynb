{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo dados gerados via método numérico\n",
    "\n",
    "Esses dados são armazenados em um arquivo HDF5 (Hierarchical Data Format 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "FILEPATH = \"/workspace/velocityHistory.h5\"\n",
    "with h5py.File(FILEPATH, \"r\") as h5f:\n",
    "    reynolds = {}\n",
    "    for key in h5f.keys():\n",
    "        reynolds[key] = h5f[key][:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representando dados de treino e validação utilizando representação latente agrupados em 3 timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo modelos de encoder e decoder do autoencoder utilizado\n",
    "\n",
    "Autoencoder 19 foi o que performou melhor, utilizando o mesmo.\n",
    "Como o arquivo .hdf5 gerado só contém os pesos, é necessário compilar o modelo novamente para utilizá-lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros do autoencoder 19\n",
    "original_dim = 63 * 63 * 2\n",
    "L1_dim = 31 * 31 * 2\n",
    "L2_dim = 15 * 15 * 2\n",
    "encoding_dim = 8 * 8 * 2\n",
    "activation_function_encoder_l1 = 'selu'\n",
    "activation_function_encoder_l2 = 'relu'\n",
    "activation_function_encoder_l3 = 'relu'\n",
    "activation_function_decoder_l1 = 'linear'\n",
    "activation_function_decoder_l2 = 'relu'\n",
    "activation_function_decoder_l3 = 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "FILEPATH = '/workspace/models/autoencoder/19.hdf5'\n",
    "autoencoder = keras.models.Sequential()\n",
    "autoencoder.add(layers.Reshape((original_dim,), input_shape=(63, 63, 2)))\n",
    "autoencoder.add(layers.Dense(\n",
    "    L1_dim, activation=activation_function_encoder_l1))\n",
    "autoencoder.add(layers.Dense(\n",
    "    L2_dim, activation=activation_function_encoder_l2))\n",
    "autoencoder.add(layers.Dense(\n",
    "    encoding_dim, activation=activation_function_encoder_l3))\n",
    "autoencoder.add(layers.Dense(\n",
    "    L2_dim, activation=activation_function_decoder_l1))\n",
    "autoencoder.add(layers.Dense(\n",
    "    L1_dim, activation=activation_function_decoder_l2))\n",
    "autoencoder.add(layers.Dense(\n",
    "    original_dim, activation=activation_function_decoder_l3))\n",
    "autoencoder.add(layers.Reshape((63, 63, 2), input_shape=(original_dim,)))\n",
    "opt = Adam(learning_rate=1e-4)\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "# Carregando pesos\n",
    "autoencoder.load_weights(FILEPATH)\n",
    "\n",
    "# Parte do encoder\n",
    "encoder_input = autoencoder.input\n",
    "# Index 2 is the last layer of the encoder\n",
    "encoder_output = autoencoder.layers[3].output\n",
    "encoder_model = Model(inputs=encoder_input, outputs=encoder_output)\n",
    "\n",
    "# Parte do decoder\n",
    "# Index 3 is the first layer of the decoder\n",
    "decoder_input = autoencoder.layers[4].input\n",
    "# Index -1 is the last layer of the decoder\n",
    "decoder_output = autoencoder.layers[-1].output\n",
    "decoder_model = Model(inputs=decoder_input, outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando dados disponíveis na representação latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reynolds_latent = {}\n",
    "for k, v in reynolds.items():\n",
    "    v_inner = v[:, 1:-1, 1:-1, :]\n",
    "    reynolds_latent[k] = encoder_model.predict(v_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupando valores latentes em grupos de 3 timesteps\n",
    "\n",
    "Input -> Conjunto de entrada com 3 valores sequenciais\n",
    "\n",
    "Output -> Quarto valor, se existir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.zeros((0, 3, 128))\n",
    "y_train = np.zeros((0, 128))\n",
    "for v in reynolds_latent.values():\n",
    "    amount_of_groups = v.shape[0] - 3\n",
    "    for i in range(amount_of_groups):\n",
    "        new_input = v[i:i+3].reshape(1, 3, 128)\n",
    "        new_output = v[i+3].reshape(1, 128)\n",
    "        x_train = np.concatenate((x_train, new_input), axis=0)\n",
    "        y_train = np.concatenate((y_train, new_output), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando 10% dos dados para validação\n",
    "\n",
    "Utiliza amostragem aleatória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "permutation_idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[permutation_idx]\n",
    "y_train = y_train[permutation_idx]\n",
    "ten_percent = int(len(x_train) * 0.1)\n",
    "x_validation = x_train[:ten_percent]\n",
    "y_validation = y_train[:ten_percent]\n",
    "x_train = x_train[ten_percent:]\n",
    "y_train = y_train[ten_percent:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerando diferentes modelos de LSTM\n",
    "\n",
    "Hiperparâmetros alterados:\n",
    "- Função de ativação das células de LSTM\n",
    "- Função de ativação da camada de saída\n",
    "- Número de neurônios da camada de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros fixos\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "timesteps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procura de hiperparâmetros utilizando _framework_ Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    activation_function_lstm = trial.suggest_categorical(\n",
    "        'activation_function_lstm', ['selu', 'relu', 'linear', 'tanh', 'sigmoid'])\n",
    "    activation_function_dense = trial.suggest_categorical(\n",
    "        'activation_function_dense', ['selu', 'relu', 'linear', 'tanh', 'sigmoid'])\n",
    "    number_of_lstm_cells = trial.suggest_int('number_of_lstm_cells', 8, 256)\n",
    "\n",
    "    lstm_network = keras.models.Sequential()\n",
    "    lstm_network.add(layers.LSTM(number_of_lstm_cells,\n",
    "                     activation=activation_function_lstm, input_shape=(None, 128)))\n",
    "    lstm_network.add(layers.Dense(128, activation=activation_function_dense))\n",
    "    opt = Adam(learning_rate=1e-4)\n",
    "    lstm_network.compile(optimizer=opt, loss='mse')\n",
    "    try:\n",
    "        history = lstm_network.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(x_validation, y_validation),\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        trial.study.stop()\n",
    "\n",
    "    path = f'/workspace/models/lstm/{trial.number}.hdf5'\n",
    "    lstm_network.save_weights(path)\n",
    "    score = history.history['val_loss'][-1]\n",
    "    # Checa se score é NaN\n",
    "    # Se sim, retorna maxfloat para desmotivar uso de hiperparâmetros que causem esse comportamento\n",
    "    if (np.isnan(score)):\n",
    "        return np.finfo(np.float32).max\n",
    "    else:\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se o estudo já existe no banco de dados, carrega-o, caso contrário, cria um novo\n",
    "try:\n",
    "    study = optuna.create_study(\n",
    "        study_name='lstm_params', directions=['minimize'], storage='sqlite:////workspace/lstm_params.db')\n",
    "except optuna.exceptions.DuplicatedStudyError:\n",
    "    study = optuna.load_study(\n",
    "        study_name='lstm_params', storage='sqlite:////workspace/lstm_params.db')\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=60 * 30)\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
